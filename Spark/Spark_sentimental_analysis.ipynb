{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import common libraries\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyspark libraries\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import  StreamingContext\n",
    "from pyspark.sql import  SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka Server configuration\n",
    "\n",
    "# Import Kafka libraries\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "\n",
    "kafka_hostname = '35.239.57.13'\n",
    "kafka_port = '9092'\n",
    "kafka_bootstrap_server = kafka_hostname + ':' + kafka_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register to Kafka as a Producer\n",
    "\n",
    "def connect_kafka_producer():\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=[kafka_bootstrap_server], api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publishes the message to Kafka Topic\n",
    "\n",
    "def publishToKafka(topic_name, data):\n",
    "    producer = connect_kafka_producer()\n",
    "    text_to_send = bytes(data, encoding='utf-8')\n",
    "    try:\n",
    "        producer.send(topic_name,value=text_to_send)\n",
    "        producer.flush()\n",
    "        print(f\"Message delivered to Kafka Topic - {topic_name}\")\n",
    "    except Exception as ex:\n",
    "        #print(\"Message sending failed!!\")\n",
    "        print(ex)\n",
    "    producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case 1\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "def sentimentCategoryBlob(score):\n",
    "    if score == 0.0:\n",
    "        category = \"neutral\"\n",
    "    elif score > 0.0:\n",
    "        category = \"positive\"\n",
    "    elif score < 0.0:\n",
    "        category = \"negative\"\n",
    "        \n",
    "    return category\n",
    "\n",
    "# Cleans the tweet Text\n",
    "def clean_tweet(text):\n",
    "    \"\"\"\n",
    "    Removes the junk characters and tweet names\n",
    "    \"\"\"\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \",text).split())\n",
    "\n",
    "# Applies Blob Algorithm to find sentiments to each texts\n",
    "def blobSentimentAnalysis(text):\n",
    "    \n",
    "    text = clean_tweet(text)\n",
    "    analysis = TextBlob(text)\n",
    "    \n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "def findBlobSentiment(text):\n",
    "    sentiment = blobSentimentAnalysis(str(text))\n",
    "    #print(sentiment)\n",
    "    category = sentimentCategoryBlob(sentiment)\n",
    "    return category\n",
    "\n",
    "def tweetsPerSentiment(dic) :\n",
    "    topic_name = \"blob_sentiments\"\n",
    "    senti_map = {\"positive\":0, \"negative\":0, \"neutral\": 0}\n",
    "    for key in dic.keys():\n",
    "        senti_map[key] = dic[key]\n",
    "    #print(dic)\n",
    "    print(senti_map)\n",
    "    if dic:\n",
    "        publishToKafka(topic_name, json.dumps(senti_map))\n",
    "        #pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case 2\n",
    "\n",
    "import re\n",
    "\n",
    "def electionMap(tweet):\n",
    "    parties_list = list()\n",
    "    if tweet: \n",
    "        tweet = tweet.lower()\n",
    "        if re.search(r\"shivsena|janata|jdu|bjp|modi|namo|chowkidar|shah|nda\", tweet):\n",
    "            parties_list.insert(0,\"BJP\")\n",
    "        if re.search(r\"indiancongress|raga|gandhi|inc|sonia|congress|rahul|priyanka|gatbandhan|nyay\", tweet):\n",
    "            parties_list.insert(0,\"INC\")\n",
    "        if re.search(r\"aap|kejri|arvind\", tweet):\n",
    "            parties_list.insert(0,\"AAP\") \n",
    "        if re.search(r\"mamata|cpi|kanhaiya|bsp|samajwadi|tmc|trinamool|dmk|mns|bjd|samajwadi|yadav|gatbandhan|naidu|kalyan\", tweet):\n",
    "            parties_list.insert(0,\"Others\")\n",
    "    return parties_list\n",
    "\n",
    "def tweetsPerParty(dic):\n",
    "    topic_name = \"election_parties\"\n",
    "    election_map = {\"BJP\": 0, \"INC\": 0, \"AAP\": 0, \"Others\": 0}\n",
    "    for key in dic.keys():\n",
    "        election_map[key] = dic[key]\n",
    "    print(election_map)\n",
    "    if dic:\n",
    "        publishToKafka(topic_name, json.dumps(election_map))\n",
    "        #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case 3\n",
    "\n",
    "# Find top 6 hashtags\n",
    "def topTrendingHashTags(records):\n",
    "    topic_name = \"trending_hashtags\"\n",
    "    top_k_tweets = {}\n",
    "    if records:\n",
    "        top_k_tweets = dict(records)\n",
    "    print(top_k_tweets)\n",
    "    if records:\n",
    "        publishToKafka(topic_name, json.dumps(top_k_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a tweet stream to JSON\n",
    "def convert_tweet(tweet):\n",
    "    if not tweet:\n",
    "        tweet = \"{}\"\n",
    "    return json.loads(tweet)\n",
    "\n",
    "# Extract Tweet Text from tweet JSON\n",
    "def extractTweetText(tweet):\n",
    "    try:\n",
    "        text = tweet['text']\n",
    "        \n",
    "    except KeyError as ex:\n",
    "        print(\"No key attribute - text in the JSON\")\n",
    "        text = \"\"\n",
    "    return text\n",
    "\n",
    "# Extract Hashtags from a tweet JSON\n",
    "def extractTweetHashtags(tweet):\n",
    "    try:\n",
    "        # hashtags is a list\n",
    "        hashtags = tweet['entities']['hashtags']\n",
    "        \n",
    "    except KeyError as ex:\n",
    "        print(\"No key attribute - entities:hashtags in the JSON\")\n",
    "        hashtags = \"\"\n",
    "        \n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) != 3:\n",
    "        print(f\"Usage: python <file_name> <netcat_host_name> <netcat_port>\")\n",
    "        exit(-1)\n",
    "\n",
    "    nc_host_name = \"localhost\"\n",
    "    nc_port = 9999\n",
    "    \n",
    "    # batch interval in seconds\n",
    "    batch_interval = 4 # batch interval\n",
    "    window_length = 15 * batch_interval # 1 min data\n",
    "    sliding_window = 1 * batch_interval # window is refreshed each second\n",
    "\n",
    "    spc = SparkContext.getOrCreate()\n",
    "    spc.setLogLevel(\"ERROR\")\n",
    "    stc = StreamingContext(spc, batch_interval)\n",
    "    stc.checkpoint(\"/tmp/checkpoint\")\n",
    "\n",
    "    tweetsDStream = stc.socketTextStream(nc_host_name, nc_port)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Converts tweet stream to json object\n",
    "        tstream = tweetsDStream.map(lambda tweet: convert_tweet(tweet))\n",
    "        #tstream = tstream.filter(lambda tweet: filterOnLang(tweet))\n",
    " \n",
    "        # Extracts the text from tweet json\n",
    "        textstream = tstream.map(lambda tweet: extractTweetText(tweet))\n",
    "        \n",
    "        # Counts the number of records in the stream batch\n",
    "        tstream.count().map(lambda x:'Tweets in this batch: %s' % x).pprint()\n",
    "        \n",
    "        # Counts the number of records in the streaming window\n",
    "        tstream.countByWindow(window_length,sliding_window).map(lambda x: \"Tweets in this 1 minute window: %s\" %x).pprint()\n",
    "        \n",
    "        ################## Use Case 2 ####################\n",
    "        \n",
    "        # Returns a map of election parties and their no. of tweets\n",
    "        election = textstream.flatMap(lambda tweet: electionMap(tweet))\n",
    "        election.countByValueAndWindow(window_length,sliding_window).foreachRDD(lambda rdd: tweetsPerParty(dict(rdd.collect())))\n",
    "      \n",
    "        ################## Use Case 3 ####################\n",
    "        \n",
    "        # Find the dictionary of top 6 Trending Hashtags in each tweets\n",
    "        hashtagstream = tstream.map(lambda tweet: extractTweetHashtags(tweet))\n",
    "        hashtagstream = hashtagstream.flatMap(lambda hashtags: [ hashtag['text'] for hashtag in hashtags if hashtag ])\n",
    "        hashtagstream.countByValueAndWindow(window_length,sliding_window).transform(lambda rdd: rdd.sortBy(lambda x: -x[1])).foreachRDD(lambda rdd: topTrendingHashTags(rdd.take(6)))\n",
    "    \n",
    "        ################## Use Case 1 ####################\n",
    "        \n",
    "        # Finds the Sentiments of the tweets using Blob based algorithm\n",
    "        sentimentstream = textstream.map(lambda text: findBlobSentiment(text))\n",
    "        sentimentstream.countByValueAndWindow(window_length,sliding_window).foreachRDD(lambda rdd: tweetsPerSentiment(dict(rdd.collect())))\n",
    "        \n",
    "        stc.start()\n",
    "        stc.awaitTermination()\n",
    "        \n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
